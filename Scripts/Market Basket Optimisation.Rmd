---
title: "Market Basket Analysis"
author: "Antony Rono"
date: "10/01/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(magrittr)
```

# Goal

We are trying to determine which products have high association i.e. those that are likely to be bought together

We will implement this using Market Basket Analysis (MBA), which uses Association Rule Mining (Apriori algorithm)

# Association Rules

-   If-then statements that helps us to show the probability of relationships between data items withIn large data sets

-   Eg

    -   *Movie Recommendation*: If they watched this, then they also watched this.
    -   *Market Basket Optimisation* : If they bought this, then they also bought this

-   Has two parts:

    1.  An antecedent (if) - An item found within the data
    2.  A consequent (then) - An item found in combination with the data

-   An association rule for items $M_1$ (antecedent) and $M_2$ (consequent) can then be expressed as:

$M_1 \rightarrow M_2$ i.e. representation of having item $M_2$ on the itemset which has $M_1$ on it

-   **Note that implication here is co-occurrence and not causality**

-   There are three metrics that help us understand the strength of the relationship (rule):

    1.  **Support** -indicates how frequently the itemset occurs

        -   Support($M_1 \rightarrow M_2$ ) = $\frac{\text{# transaction containing $M_1$ and $M_2$}}{\text{# transaction}}$

    2.  **Confidence** - the number of times the rule is found to be true (i.e. likelines of occurrence of consequent given the the antecedent has occurred)

    -   Confidence($M_1 \rightarrow M_2$ ) = $\frac{\text{# transaction containing $M_1$ and $M_2$}}{\text{# transaction containing $M_1$ }}$

    3.  **Lift** - How many times the rule is expected to be found true

        -   Ratio of confidence to the baseline probability of occurrence of the consequent
        -   Think of it as the *lift* that $M_1$ provides to our confdence for having $M_2$ on the itemset
        -   IOW: lift is the rise in probability of having $M_2$ on the cart with the knowledge of $M_1$ being present over the probability of having $M_2$ on the cart without any knowledge about presence of $M_1$

    -   Lift($M_1 \rightarrow M_2$ ) =$\frac{\text{confidence}(M_1 \rightarrow M_2)}{\text{support}(M_2)}$ = $\frac{\frac{\text{# user transactions containing $M_1$ and $M_2$}}{\text{# user transactions containing $M_1$}}}{\text{# user transactions containing $M_2$}}$

    -   In cases where $M_1$ actually leads to $M_2$, value of lift will be greater than 1

    -   Generally:

        -   A value of lift greater than 1 vouches for high association i.e. having $M_1$ on the itemset increases the chances of having $M_2$ on the itemset
        -   If the value is positive, there is a positive correlation
        -   If the value is negative, there is a negative correlation
        -   If the value is 1, there is no correlation

-   The general worflow is:

    1.  Generate an itemset from a list all items
    2.  Generate a rule for each item set

# Importing Data

```{r results = 'hide'}

file_path <- "Import files/Market_Basket_Optimisation.csv"

data <-  read_csv(file_path, col_names = FALSE)

```

Each row represents the transactions for individual customers

# Data Preprocessing

We convert the data frame to a sparse matrix (called transactions)

Sparse matrix is a matrix of 0s and 1s, with each row and column representing the various products

```{r}
library(arules)

dataset <- read.transactions(file_path, sep = ",", rm.duplicates = TRUE )

summary(dataset)

```

# Creating a frequency plot of the sparse matrix (Top 20 most bought products)

```{r}
library(RColorBrewer)
itemFrequencyPlot(dataset,
   topN=20,
   col=brewer.pal(8,'Pastel2'),
   main='Relative Item Frequency Plot',
   type="relative",
   ylab="Item Frequency (Relative)") 

```

# Training the Apriori Model on the dataset

The basic steps in implementing the Apriori algorithm are as follows:

1.  Set up a minimum support and confidence

2.  Take all the subsets in transactions having higher support than the minimum support

3.  Take all the subsets in transactions having higher confidence than the minimum confidence

4.  Sort the rules by decresing lift

The choice of support(how frequently the item appears in your data set) and confidence (frequency of the rule) varies by business case: depends on the goal, data size etc

For minimum support, we want products that are bought at least two times a day i.e. 2\*7/len(dataset).

Minimum length is an specifies the minimum number of products you'd like to have in your rule (Not mandatory to include this)

Maximum length specifies the maximum number of products you'd like to have in your rule (Not mandatory to include this)

```{r}

rules <- apriori(dataset, 
                 parameter = list(support = 14/nrow(dataset),
                                  confidence = .2,
                                  minlen = 2,
                                  maxlen = 20
                                  )
                 )

# Removing Redundant Rules
#rules <- rules[!is.redundant(rules)]
subset.rules <- which(colSums(is.subset(rules, rules)) > 1) # get subset rules in vector
rules <- rules[-subset.rules] # remove subset rules.
```

# Rules Arranged by Lift (i.e. starting with those with high association)

```{r}
rules_df <- DATAFRAME(rules) %>% arrange(desc(lift))

DT::datatable(rules_df)
```

# Visualizing the results (rules)

## Relationship between support, confidence and lift

```{r}
library(arulesViz)

plot(rules,jitter = 0, engine = "plotly")

```

-   Rules with high confidence tend to have low support, and vice versa

-   Rules with high lift tend to have relatively low support

## Relationship between support, confidence and Number of items in the rule (order)

```{r}
plot(rules, method = "two-key plot")
```

-   The order and support have a very strong inverse relationship i.e. as order increases, the support decreases

## Network Graph Visualisation

```{r}

subrules <- head(sort(rules, by="lift"), n = 30, by = "lift")

#plot(subrules, method = "graph", engine = "htmlwidget")

plot(subrules, method = "graph",
     control = list(
       # edges = ggraph::geom_edge_link(
       #   end_cap = ggraph::circle(4, "mm"),
       #   start_cap = ggraph::circle(4, "mm"),
       #   color = "black",
       #   arrow = arrow(length = unit(2, "mm"), angle = 20, type = "closed"),
       #   alpha = .2
       # ),
       nodes = ggraph::geom_node_point(aes_string(size = "support", color = "lift"))
       #nodetext = ggraph::geom_node_label(aes_string(label = "label"), alpha = .8, repel = TRUE)
     )
) +
  scale_color_gradient(low = "dodgerblue", high = "red") +
  scale_size(range = c(2, 10))


```

```{r}
# Shiny App for Interactive Manipulations and Visualization
#ruleExplorer(subrules, sidebarWidth = 2, graphHeight = '600px')
```
